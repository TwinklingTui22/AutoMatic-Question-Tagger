# -*- coding: utf-8 -*-
"""AIproject (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11PM4P85Bio0HebI3vJiuy2Q2iLAG02ft
"""

import pandas as pd
import pickle

df=pd.read_csv('https://raw.githubusercontent.com/dhanush5120/Autonomus-Tagging-of-Stackoverflow-Questions/master/stackoverflowtags.csv')

import re  
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords 
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer


df.title=df.title.drop_duplicates(keep='first')

def clean(s):
    s=str(s)
    s=s.lower()
    html=re.compile('<.*?>')   #removing html tags
    cleaned = re.sub(html,' ',s)
    fil=[]
    for i in cleaned.split():    # splits the text and repalces the unwanted characters with ''
        if i!='c++':
            cleaned=re.sub('[^A-Za-z]', '', i) #search the pattern !(A-Z & a-z) and replace with ''
            fil.append(cleaned)
        else:
            fil.append(i)
    return fil         # returns the splitted text with removed stopwords and html tags

stop=set(stopwords.words('english'))  
sno=SnowballStemmer('english')
print("cleaned text is:\n", clean(df.title[0]))          # function ca

def stem(s):
    fil=[]
    for i in s:
        if i not in stop:
            s=(sno.stem(i).encode('utf8'))  # encoding the data into a clean file
            fil.append(s)
    s=b' '.join(fil)
    return s

ques=[]
for j in df.title:
    ques.append(stem(clean(j)))
df['cleanQues'] = ques

import re
ctags=[]
for i in df.tags:
    ctags.append(re.sub('[^A-Za-z#+-]', ' ', i)) # search the pattern !(A-Z & a-z) and replace with ''
df['cleanTags']=ctags

d=pd.DataFrame()
d['text']=df.cleanQues
d['tags']=df.cleanTags
d.to_csv('datafinal',index=False)
df = pd.read_csv('datafinal')

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(df.text, df.tags, test_size=0.2, random_state=9)

from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
tfvectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, smooth_idf=True, norm="l2",
                             tokenizer = lambda x: x.split(), sublinear_tf=False, ngram_range=(1,3))
x_train_multilabel = tfvectorizer.fit_transform(x_train)
x_test_multilabel = tfvectorizer.transform(x_test)

vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary=True) # problem with binary = 'true'

y_train_multilabel = vectorizer.fit_transform(y_train)
y_test_multilabel = vectorizer.transform(y_test)

from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn import metrics
from sklearn.metrics import f1_score,precision_score,recall_score

# log -> log_loss
classifier = OneVsRestClassifier(SGDClassifier(loss='log_loss', max_iter = 5, tol = None, alpha=0.00001, penalty='l1'), n_jobs=-1)
classifier.fit(x_train_multilabel, y_train_multilabel)
predictions = classifier.predict(x_test_multilabel)


import re   # importing regular expressions used for cleaning texts

# importing natural language toolkit
# that helps in cleaning texts by using
# stopwords, SnowballStemmer, WordNetLemmatizer libraries

from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
def clean(s):
    s=str(s)
    s=s.lower()
    html=re.compile('<.*?>')   #removing html tags
    cleaned = re.sub(html,' ',s)
    fil=[]
    for i in cleaned.split():     # splits the text and repalces the unwanted characters with ''
        if i!='c++':
            cleaned=re.sub('[^A-Za-z]', '', i)     #search the pattern !(A-Z & a-z) and replace with ''
            fil.append(cleaned)
        else:
            fil.append(i)
    return fil        # returns the splitted text with removed stopwords and html tags


stop=set(stopwords.words('english'))    #loading stopwords in english to compare and remove
sno=SnowballStemmer('english')


# function to stem the data
# stemming means grouping the words
# after cleaning the data for further processing

def stem(s):
    fil=[]
    for _ in s:
        if _ not in stop:
            s=(sno.stem(_).encode('utf8'))   # encoding the data into a clean file
            fil.append(s)
    s=b' '.join(fil)
    return s


def final_prediction(q):
    """
    Returns final prediction
    """
    pickled_model = pickle.load(open('model2.pkl', 'rb'))

    l = []
    l.append(stem(clean(q)))
    x = tfvectorizer.transform(l)
    t = pickled_model.predict(x)
    k = vectorizer.inverse_transform(t)
    print("k is:")
    print(k, type(k))
    print(len(k))

    if len(k[0]) > 0:
        return k[0] 
    else:
        return ["Sorry couldn't find tag"]




